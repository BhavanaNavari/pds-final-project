{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "MTXhPLL2VAGI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)  # specific seed value"
   ],
   "metadata": {
    "id": "5V119IpJmxt5",
    "ExecuteTime": {
     "end_time": "2023-12-09T04:24:28.127050900Z",
     "start_time": "2023-12-09T04:24:27.981613300Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "wuQGmXOVmymI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Data Science Jobs Salaries.csv')\n",
    "\n",
    "# Explore the first few rows of the dataset\n",
    "print(df.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SW-z9R1JVAQx",
    "outputId": "3aa80999-3e5e-436a-9466-e6708167f191",
    "ExecuteTime": {
     "end_time": "2023-12-09T04:24:28.351123200Z",
     "start_time": "2023-12-09T04:24:27.997397Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data Science Jobs Salaries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Load the dataset\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mData Science Jobs Salaries.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Explore the first few rows of the dataset\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mhead())\n",
      "File \u001B[1;32m~\\Downloads\\bigdataflaskproject\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\bigdataflaskproject\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\Downloads\\bigdataflaskproject\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\bigdataflaskproject\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\Downloads\\bigdataflaskproject\\venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data Science Jobs Salaries.csv'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "gf7ZAUcvOUhS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Task 1: Convert 'work_year' to a consistent format\n",
    "df['work_year'] = df['work_year'].str.replace('e', '', regex=False).astype(int)\n",
    "\n",
    "# Task 2: Convert 'salary_currency' to uppercase for consistency\n",
    "df['salary_currency'] = df['salary_currency'].str.upper()\n",
    "\n",
    "# Task 3: Convert 'experience_level' to a categorical data type\n",
    "df['experience_level'] = pd.Categorical(df['experience_level'], categories=['EN', 'MI', 'SE', 'EX'], ordered=True)\n",
    "\n",
    "# Task 4: Convert 'remote_ratio' to a percentage\n",
    "df['remote_ratio'] = df['remote_ratio'] / 100.0\n",
    "\n",
    "# Task 5: Handle missing values (replace with mean or other strategies)\n",
    "df['salary'].fillna(df['salary'].mean(), inplace=True)\n",
    "\n",
    "# Task 6: Drop unnecessary columns\n",
    "df.drop(['salary_in_usd'], axis=1, inplace=True)\n",
    "\n",
    "# Display the preprocessed dataset\n",
    "print(\"\\nPreprocessed Dataset:\")\n",
    "print(df.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7YlR6srOU0Y",
    "outputId": "eb043438-d1c1-47cc-92c3-4f0a9a7ad3c9",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Task 1: Identify Big Data Job Families**"
   ],
   "metadata": {
    "id": "L1SWc5ThVU8T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the unique job titles to understand the data\n",
    "unique_job_titles = df['job_title'].unique()\n",
    "print(\"Unique Job Titles:\")\n",
    "print(unique_job_titles)\n",
    "\n",
    "# Task 1: Identify Big Data Job Families\n",
    "# Assume keywords related to Big Data job families (customize this based on your data)\n",
    "big_data_keywords = ['Data Scientist', 'Machine Learning', 'Data Analyst', 'Data Engineer', 'Data Science Engineer', 'Data Analytics', 'ML Engineer', 'AI Scientist', 'Big Data', 'Data Architect']\n",
    "\n",
    "# Create a new column 'job_family' and assign it based on the keywords\n",
    "def assign_job_family(title):\n",
    "    for keyword in big_data_keywords:\n",
    "        if keyword.lower() in title.lower():\n",
    "            return keyword\n",
    "    return 'Other'\n",
    "\n",
    "df['job_family'] = df['job_title'].apply(assign_job_family)\n",
    "\n",
    "# Display the updated dataset with the 'job_family' column\n",
    "print(\"\\nUpdated Dataset with Job Families:\")\n",
    "print(df[['job_title', 'job_family']].head())\n"
   ],
   "metadata": {
    "id": "iGM_VtcFPAHb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7298c84-795e-469f-82aa-b21bfbdb1e51",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Task 2: Extract Big Data Skills**"
   ],
   "metadata": {
    "id": "LyU3JF08Bb33"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# Download the 'stopwords' corpus\n",
    "nltk.download('stopwords')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOV-BQzaCjZF",
    "outputId": "c261cdcf-98cb-4ee3-b9b8-5aa1294756b8",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Combine all job titles into a single string\n",
    "all_job_titles = ' '.join(df['job_title'].str.lower())\n",
    "\n",
    "# Split the string into words\n",
    "words = all_job_titles.split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_skills = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Extract unique words as potential skills\n",
    "unique_skills = set(filtered_skills)\n",
    "\n",
    "# Convert the list of skills to a DataFrame\n",
    "skills_df = pd.DataFrame({'Big Data Skills': list(unique_skills)})\n",
    "\n",
    "# Display the refined list of extracted skills in table form\n",
    "print(\"\\nRefined Extracted Big Data Skills:\")\n",
    "print(skills_df)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pspMdYlGBcCP",
    "outputId": "0193dc9f-ea2b-4406-ed25-aba02bca2ee1",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Task 3: Homogeneous Groups of Big Data Skills (Cluster Analysis or Association Analysis)**\n",
    "You may use machine learning clustering techniques or association rules mining to identify groups of skills.\n",
    "This step can be more complex and depends on the specific techniques you choose.\n"
   ],
   "metadata": {
    "id": "cWFqKsNID7Qr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "NYiw4S-LEK_P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ],
   "metadata": {
    "id": "4gYMYJ2hEMJH",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use TF-IDF vectorization to convert skills into numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "skills_tfidf = vectorizer.fit_transform(skills_df['Big Data Skills'])\n",
    "\n",
    "# Apply KMeans clustering\n",
    "num_clusters = 3  # You can adjust the number of clusters based on your preference\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "skills_df['Cluster'] = kmeans.fit_predict(skills_tfidf)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nHomogeneous Groups of Big Data Skills (Cluster Analysis):\")\n",
    "print(skills_df.sort_values(by='Cluster'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMWpKO78D6Zd",
    "outputId": "0d3f3a02-b095-4593-d483-fc1de1d80159",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "clustering results have grouped the skills into three clusters (0, 1, and 2). Each skill is assigned to a cluster, and you can interpret these clusters as homogeneous groups of Big Data skills."
   ],
   "metadata": {
    "id": "38OX-jkhEqUd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Task 4: Characterize Job Families by Competence Level**\n",
    "Assuming 'experience_level' column can be used to characterize competence levels\n",
    "competence_levels = df.groupby('job_title')['experience_level'].unique() print(\"Competence Levels by Job Family:\") print(competence_levels)"
   ],
   "metadata": {
    "id": "_5RCGj7hE01r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Group by job title and collect unique competence levels\n",
    "competence_levels = df.groupby('job_title')['experience_level'].unique()\n",
    "\n",
    "# Display the competence levels by job family\n",
    "print(\"\\nCompetence Levels by Job Family:\")\n",
    "for job_title, levels in competence_levels.items():\n",
    "    print(f\"{job_title}: {', '.join(levels)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr2_zdM5E5Ey",
    "outputId": "57b18510-0154-4e2f-b100-fe3281a639cb",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.191057600Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*In this section, we characterized job families in the Big Data field based on the provided dataset. The analysis involved identifying homogeneous groups of Big Data skills through a clustering approach, revealing three distinct skill clusters. Subsequently, we delved into the competence levels associated with each job family using the 'experience_level' column. The results unveiled a comprehensive mapping of competence levels to specific job titles, offering valuable insights into the expertise required for various roles in the Big Data domain. This categorization facilitates a nuanced understanding of the skills and experience levels demanded by different job families, serving as a valuable resource for HR departments seeking to optimize recruitment strategies in the ever-evolving landscape of Big Data professions.*"
   ],
   "metadata": {
    "id": "Zzx6zzqyGT4s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Task 5: Data Analysis and Visualization**\n",
    "You can use libraries like Matplotlib or Seaborn for visualization.\n",
    "Analyze and visualize the distribution of competence levels, most valued skills, etc."
   ],
   "metadata": {
    "id": "Cg2gC1DXGeiA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Distribution of Competence Levels:**\n",
    "\n",
    "Visualizing the distribution of competence levels across all job families can provide insights into the overall workforce composition."
   ],
   "metadata": {
    "id": "MGB6iigUJfHg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#\n",
    "\n",
    "# Distribution of Competence Levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='experience_level', data=df, order=df['experience_level'].value_counts().index)\n",
    "plt.title('Distribution of Competence Levels')\n",
    "plt.xlabel('Experience Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "0z6UObD5HH_V",
    "outputId": "75f83d53-2df7-4ef9-80f0-34916f850f74",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.208536Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "vWCbYdV-Jdv9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The visualization is a bar plot showing the distribution of competence levels based on the 'experience_level' column.*"
   ],
   "metadata": {
    "id": "VI9NhlDYH2x8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Most Valued Skills\n",
    "#'job_title' column contains information about skills\n",
    "all_job_titles = ' '.join(df['job_title'].str.lower())\n",
    "words = all_job_titles.split()\n",
    "filtered_skills = [word for word in words if word not in stopwords.words('english')]\n",
    "unique_skills = set(filtered_skills)"
   ],
   "metadata": {
    "id": "dSJM8HlvHmp7",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Most Valued Skills Word Cloud:**\n",
    "\n",
    "Creating a word cloud can visually represent the most frequently occurring skills across all job titles, providing a quick overview of the prominent skills."
   ],
   "metadata": {
    "id": "zxBAOH58JvAW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting Word Cloud for Most Valued Skills\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(unique_skills))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Most Valued Skills')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "VAuEbsUWHips",
    "outputId": "a65a0fe9-8c58-42be-a849-517f26db500b",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*visualization is a word cloud depicting the most valued skills extracted from the 'job_title' column.*"
   ],
   "metadata": {
    "id": "uLrQeDF5H8n0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Competence Levels by Job Family Heatmap:**\n",
    "\n",
    "A heatmap displayin the association between job families and competence level"
   ],
   "metadata": {
    "id": "F686gyB1I6rC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a pivot table for heatmap\n",
    "heatmap_data = df.groupby(['job_title', 'experience_level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting Heatmap for Competence Levels by Job Family\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, cmap='viridis', annot=True, fmt='d')\n",
    "plt.title('Competence Levels by Job Family')\n",
    "plt.xlabel('Experience Level')\n",
    "plt.ylabel('Job Family')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "pVa1112ZIjiV",
    "outputId": "4bc6b294-8f4d-4e60-b476-cc6a6b62dcde",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Task 6: Recommendations for Recruitment**\n",
    "Based on your analysis, provide recommendations for recruitment strategies."
   ],
   "metadata": {
    "id": "2ApDvY44LYdy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tabulate"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qutwJTvNOVoD",
    "outputId": "cd829c70-d32e-4aa0-eaa0-d359c07a0002",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "# Calculate the frequency of each skill in the dataset\n",
    "skill_frequencies = {skill: filtered_skills.count(skill) for skill in unique_skills}\n",
    "\n",
    "# Rank the skills based on frequency (1 is the highest rank)\n",
    "ranked_skills = sorted(skill_frequencies, key=skill_frequencies.get, reverse=True)\n",
    "numbered_skills = [{'Skill': skill, 'Rank': i+1} for i, skill in enumerate(ranked_skills)]\n",
    "\n",
    "# Display the list of skills with ranks in a visually attractive table\n",
    "table = tabulate(numbered_skills, headers=\"keys\", tablefmt=\"pretty\")\n",
    "print(table)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJN8KS3MLUbV",
    "outputId": "eb483227-e8e2-42d9-8f98-b8ab7f150068",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "# Assuming 'job_title' column contains information about job families\n",
    "job_families = df['job_title'].str.lower()\n",
    "\n",
    "# Calculate the frequency of each job family in the dataset\n",
    "job_family_frequencies = job_families.value_counts().to_dict()\n",
    "\n",
    "# Rank the job families based on frequency (1 is the highest rank)\n",
    "ranked_job_families = sorted(job_family_frequencies, key=job_family_frequencies.get, reverse=True)\n",
    "numbered_job_families = [{'Job Family': job_family, 'Rank': i+1} for i, job_family in enumerate(ranked_job_families)]\n",
    "\n",
    "# Display the list of job families with ranks in a visually attractive table\n",
    "table = tabulate(numbered_job_families, headers=\"keys\", tablefmt=\"pretty\")\n",
    "print(table)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDoh4OspOeoj",
    "outputId": "1999372c-feaf-42e4-8d8a-a4156aa9ffc2",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Based on your analysis, provide recommendations for recruitment strategies.**\n",
    "\n"
   ],
   "metadata": {
    "id": "LBMMlaHXP14l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Focus on In-Demand Job Families:***\n",
    "\n",
    "*Prioritize recruitment efforts for job families that are highly prevalent and in demand, such as \"Data Scientist,\" \"Data Engineer,\" and \"Machine Learning Engineer.\"*\n",
    "\n",
    "***Emphasize Key Skills in Recruitment:***\n",
    "\n",
    "*Identify and emphasize the most valued skills in the Big Data domain, as highlighted by the word cloud and skill ranking analysis.*\n",
    "*Tailor recruitment messages to highlight the importance of these key skills for the specific job families.*\n",
    "\n",
    "***Consider Remote Work Opportunities:***\n",
    "\n",
    "*Acknowledge the trend of remote work, as indicated by the \"remote_ratio\" column in the dataset.*\n",
    "*Consider offering remote work opportunities or flexibility to attract a broader pool of candidates.*\n",
    "\n",
    "***Target Mid-Level and Senior-Level Candidates:***\n",
    "\n",
    "*Given the distribution of experience levels in the dataset, target mid-level (MI) and senior-level (SE) candidates for key roles.*\n",
    "*Develop recruitment strategies that align with the experience level preferences of each job family.*\n",
    "\n",
    "***Invest in Skill Development Programs:***\n",
    "\n",
    "*Recognize the importance of specific skills and competencies for various job families*.\n",
    "*Develop tailored skill acquisition programs or training sessions to enhance these skills among potential candidates, as highlighted by the word cloud and skill ranking.*\n",
    "\n",
    "***Utilize Online Platforms for Job Posts:***\n",
    "\n",
    "*Leverage online platforms and job boards for posting job opportunities to reach a wider audience.*\n",
    "*Craft compelling job descriptions that clearly outline the skills and qualifications required for each role.*\n",
    "\n",
    "***Collaborate with Educational Institutions:***\n",
    "\n",
    "*Establish partnerships with educational institutions to tap into emerging talent.*\n",
    "*Collaborate with universities and training programs to identify and attract candidates with relevant skills.*\n",
    "\n",
    "***Stay Competitive with Compensation:***\n",
    "\n",
    "*Stay informed about industry salary trends and offer competitive compensation packages.*\n",
    "*Highlight additional benefits, such as professional development opportunities and a positive work culture.*\n",
    "\n",
    "***Promote Diversity and Inclusion:***\n",
    "\n",
    "*Foster a diverse and inclusive workplace by actively seeking candidates from diverse backgrounds.*\n",
    "*Highlight the company's commitment to diversity in recruitment materials.*\n",
    "\n",
    "***Continuous Monitoring and Adjustment:***\n",
    "\n",
    "*Regularly monitor recruitment outcomes and adjust strategies based on the evolving needs of the organization and the job market.*\n",
    "\n",
    "\n",
    "*These recommendations aim to enhance the effectiveness of recruitment strategies, aligning them with the specific characteristics of the Big Data job market and the preferences of potential candidates.*"
   ],
   "metadata": {
    "id": "BCfPEOhWo2uS"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Rx_A9fiDsZCy",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Import libraries\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# Assuming you have extracted skills and stored them in the 'extracted_skills' column\n",
    "df['extracted_skills'] = df['job_title'].str.lower().str.split()\n",
    "\n",
    "# Flatten the list of lists into a single list of skills\n",
    "all_skills = [skill for skills_list in df['extracted_skills'] for skill in skills_list]\n",
    "\n",
    "# Create a DataFrame with skills and their frequencies\n",
    "skill_frequencies_df = pd.DataFrame(all_skills, columns=['skill']).value_counts().reset_index(name='frequency')\n",
    "\n",
    "# Job Families Overview\n",
    "fig1 = px.pie(df, names='job_title', title='Job Families Overview')\n",
    "\n",
    "# Skill Clusters and Rankings\n",
    "fig2 = px.bar(skill_frequencies_df, x='skill', y='frequency', title='Skill Clusters and Rankings')\n",
    "\n",
    "# Display the plots\n",
    "fig1.show()\n",
    "fig2.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CQ1BRTrnt5qR",
    "outputId": "aabfb2ea-8d9d-47d7-eee0-71c59b634566",
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-09T04:24:28.210438200Z"
    }
   }
  }
 ]
}
